(15, 3000w?)

How well does the implementation work? (as judged from the description in the
dissertation, and the demo)

What is the quality, sophistication and difficulty of the software (or
conceptual) implementation as judged from the description in the dissertation,
and the demo?

Is the programming (or modelling) style adequate and of good quality
(efficient, reusable, modular, clear code)?

Is the documentation in the code appropriate and useful?


\subsection{Implementation}

\todo{intro?}

Not mentioning output in detail


\subsubsection{User Interface}

User interface is a low-priority requirement and needs to provide minimal
interactivity, therefore command line interface was deemed appropriate. Ruby
provides many command line commands. However, to comply with a common practice
for ruby applications it was decided to use \textit{Rake} \parencite{Rake}, a
software task managament tool to make all application tasks easily available to
users. A full set of instructions for using Rake and the available commands is
available in the User Manual in Appendix \ref{sec:user_manual}.

User input is provided in a text file which is parsed and instantiates the
environment and other simulation objects. Some care has been taken to provide
data validation to ensure a successful simulation and to give users feedback
about what expected common input mistakes. The full guide to using an input
file is given in the User Manual in Appendix \ref{sec:user_manual}.

\todo{include validation specs in maintenance manual?}


\subsubsection{The Environment}




\subsubsection{Road Network}
\label{sec:design:network}

A real world road network is represented as a weighted connected graph
constructed of edges (roads) and vertices (origins/destinations). Each vertex
has associated properties: a list of connected edges with their lengths
(weights), a passenger and a taxi (if present). Taxis can travel between
connected vertices, and are assumed to take the shortest routes and travel at a
constant speed. Passengers appear at nodes and when taxi is at a node  they can
interact.

Road networks have a specific structure that is different from generic randomly
generated graphs. \textcite{Eisenstat2011graphs+quadtree} believes that the
structure of the graphs is important for optimisation problems, giving the
example of optimising logistics operations for a fleet of vehicles where
algorithmic performance greatly differs from that on generic graphs. Uniform
planar graphs are a reasonably realistic way to represent road networks
\parencite{Eisenstat2011graphs+quadtree, Masucci2009graphs+london}. A way to
generate random planar graphs is suggested by
\textcite{Brinkmann2007graphs+generate}, including software called
\textit{plantri}.

Shortest paths in graphs can be calculated by Djikstra's algorithm
\parencite{Cormen2009algorithms}. However, calculating all possible shortest
paths is not necessary as some routes could never be travelled. Furthermore,
that may be unfeasible in a very large network as the number of paths grows
exponentially. Online calculation of a shortest path between two nodes in a
graph can be efficiently done using \textit{A-star} algorithm by
\textcite{Hart1968paths}.

The network was implemented by using a graphing gem \textit{plexus}
\parencite{Plexus}. An alternative graph library considered was \textit{RGL}
\parencite{Rgl}. Unlike RGL, Plexus had not implemented an algorithm to check
for connectivity of directed graphs which could potentially be an issue.
However, RGL was no longer in active development and Plexus natively supports
numerous graph algorithms, including A-star shortest path search that was
mentioned above.


\subsubsection{Taxi}

The taxi agent is arguably the most important component of the simulation. It
stores its location and reachable destinations in the environment, the last
reward received, a list of possible prices, and its costs. Taxi initialises the
Q-Learner (see Section \ref{sec:implementation:ai}) with an initial state and a
list of available actions that depend on the location, list of available prices
and presence of a passenger.

A taxi is called to act by the World, when it processes an action and updates
internal parameters. The Q-Learner is updated with the reward for taking the
action, and the taxi is marked as engaged for as long as the action takes. Then
taxi's location and total profit are updated, and it is decided if the new
location has a passenger present. A new action is then chosen by the Q-Learner,
which will be executed the next time the taxi is called to act.

Action is a subclass of Taxi. Each action has a known type: \textit{wait},
\textit{drive} to a location without transporting a passenger or \textit{offer}
a fare for transporting a passenger to a location. All of the action types have
a known \textit{fixed cost}, \textit{variable cost} (not used for
\textit{wait}) and \textit{units} of time the action takes. The costs and units
are used to calculate a cost for action. \textit{Drive} or \textit{offer}
actions additionally have a known destination, and \textit{offer} action can
has different costs depending on whether the offer was accepted by a passenger.
When the taxi starts an action, it is marked as busy.

Each location has two states that the taxi can discern -- without a passenger
present or with a passenger going to a certain location. Each state is
essentially a tuple of \(\left \langle location, destination \right \rangle\)
where the destination can be empty.


\subsubsection{Q-Learner}
\label{sec:implementation:ai}

\todo{Subclass of taxi}

\todo{Move section to architecture or requirements} State is based on the
origin, passenger and intended destination. Actions a taxi can take is
enquiring a passenger for destination, offering a price or driving to another
place. If the passenger accepts the price, taxi drives the passenger to the
destination and receives a reward. Keeping track of the exact passenger is not
important. Each time step has a negative reward based on what action the taxi
is taking, always depending on time but also could depend on distance
travelled. The probability of the passenger accepting the fare is the
transition probability. This specifies the research problem as a finite MDP
according to the definition in Section \ref{sec:literature:ai:mdp} (although it
can be argued that prices could theoretically be a continuous space, in reality
they are not).

As described in section \ref{sec:requirements:ai}, Q-Learner is a simple and
adequate reinforcement learning method fit for the project. Of course, the very
basic Q-Learner will need to be extended to accomodate exploration in this
paricular case of having an active agent (see Section
\ref{sec:literature:ai:exploration}. Therefore an exploration constant
\(\varepsilon\) needs to be used for action selection, and it can stay fixed as
the horizon for the simulation will normally be unknown.

A further addition required for the Q-Learner is a step-size function. This
function is used to adjust for the bias of previous estimates. As a state is
being visited more and more, any new experiences are less and less important
compared to the old experiences, therefore the TD weights should be adjusted
less. This issue was discussed in more detail by
\textcite{Sutton1994ai+stepsize}, where three different algorithms are
introduced for optimising the step sizes for improved reinforcement learning
performance. Nevertheless, the policy will still converge even if the step size
simply decays (i.e \(\lim_{n \to \inf}\alpha(n) = 0\) where \(n\) is the number
of times a state has been visited). Therefore for simplicity the function can
be fixed as \(\alpha(n)=\frac{1}{n}\).

When the step size function is added to a basic Q-Learner, the algorithm
adapted from \textcite{Sutton1998ai+reinforcement} can be used as shown in
Algorithm \ref{algorithm:q}. This algorithm updates action-state pair value
estimates, and the learner can then select the optimal action depending on the
exploration function (action selection is not shown here). The required inputs
for initialisation and for further operation of the algorithm are clearly
listed.

\begin{algorithm}
  \caption{
  Q-learning. Algorithm that needs to be called after each transition. 
  Adapted from \textcite{Sutton1998ai+reinforcement}. 
  \label{algorithm:q}}

  \begin{algorithmic}[1]
    \Require
      \Statex $s$ is the last state,
      \Statex $s'$ is the next state,
      \Statex $a$ is the last action,
      \Statex $A(s)$ is a set of actions for a state,
      \Statex $R$ is the immediate reward received,
      \Statex $Q$ is an array hosting the current action-value estimate
      \Statex $H$ is the visit history of state-action pairs,
      \Statex $\alpha$ is a step-size function,
      \Statex $\gamma$ is a discount factor.
      \Statex Policy $Q(s, a)$ is initialised arbitrarily
      \Statex History $H$ is empty
    \Function{Q\_learning}{$s,a,R,a'$}
      \State $\delta \gets R + 
              \gamma \cdot max_{a' \in A(s')} Q(s', a') - Q(s, a)$
      \State $Q(s, a) \gets Q(s, a) + \alpha \cdot \delta$
      \State $H(s, a) \gets H(s, a) + 1$
      \State \Return $Q, H$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\subsubsection{Passenger}

As was identified in Section \ref{sec:requirements:passenger}, passengers have
a set of characteristics and can passively interact with taxis. As each
characteristic is potentially a complex object with different values and
probability distributions, it was decided to implement it as a subclass of
Passenger class. Passengers agents are generated on demand if a taxi is
present, according to a location's probability of having a passenger.

These software classes were designed with the goal of eventually having user-
customisable passenger details and characteristics. However, due to time
pressure and higher priority goals, the passenger details and characteristics
are fixed and uniform.


\subsubsection{Implementation Issues}

Constants are not transparent

Process not entirely realistic as passenger is set and action is chosen before
the previous action is completed
