\todo{remove notes}

(15, 3000w?)

How well does the implementation work? (as judged from the description in the
dissertation, and the demo)

What is the quality, sophistication and difficulty of the software (or
conceptual) implementation as judged from the description in the dissertation,
and the demo?

Is the programming (or modelling) style adequate and of good quality
(efficient, reusable, modular, clear code)?

Is the documentation in the code appropriate and useful?


\subsection{Implementation}
\label{sec:implementation:software}

This section explains how the system was implemented following the software
architecture specified in Figure \ref{figure:design:software} and its detailed
description in Section \ref{sec:design:architecture}. The software components
are: User Interface (Section \ref{sec:implementation:software:ui}), Road
Network (\ref{sec:implementation:software:roads}), Passenger
(\ref{sec:implementation:passenger}), Taxi
(\ref{sec:implementation:software:taxi}) and Q-Learner
(\ref{sec:implementation:ai}). Besides a detailed look at the software modules,
this section is concluded by an overview of known limitations and issues with
the current implementation in Secton \ref{sec:implementation:software:issues}.


\subsubsection{User Interface: Input and Output}
\label{sec:implementation:software:ui}

User interface is a low-priority requirement and needs to provide minimal
interactivity, therefore command line interface was deemed appropriate. Ruby
provides many command line commands. However, to comply with a common practice
for ruby applications it was decided to use \textit{Rake} \parencite{Rake}, a
command-line software task management tool to make all application tasks easily
available to users. A full set of instructions for using Rake and the available
commands is available in the User Manual in Appendix
\ref{sec:user_manual:using:interface}.

User input is provided in a text file which is parsed and instantiates the
environment and other simulation objects. Some care has been taken to provide
data validation to ensure a successful simulation and to give users feedback
about expected common input mistakes. Unfortunately due to time constraints the
data validation protection is fragmentary, so in some cases it may be harder
for users to trace the wrongly input data. The full guide to using an input
file is given in the User Manual in Appendix
\ref{sec:user_manual:using:inputs}.

Simulation output is provided in log files, important data being written with
each iteration of the simulation. The output can easily be configured and
changed by developers following the explanation in the Maintenance Manual in
Appendix \ref{sec:maintenance_manual:customising_output}. The output logging is
a module that can be reused in other source files as needed.


\subsubsection{The Environment}
\label{sec:implementation:software:environment}

The environment is initialised based on the user input as discussed in the User
Manual in Appendix \ref{sec:user_manual:using:inputs}. The environment in turn
initialises all simulation objects and agents, and runs the simulation twice:
once using a range of fare prices, and once to establish a benchmark with a
fixed taxi fare price. To reduce software coupling, the implementation also
abstracts some methods of the road network that are used by taxis and
passengers.


\subsubsection{Road Network}
\label{sec:implementation:software:roads}

A real world road network is represented as a weighted connected graph
constructed of edges (roads) and vertices (origins/destinations). Each vertex
has associated properties: a list of connected vertices with their connection
distances (weights), a passenger and a taxi (if present). Taxis can travel
between connected vertices, and are assumed to take the shortest routes and
travel at a constant speed. Passengers appear at nodes and when taxi is at a
node  they can interact.

Road networks have a specific structure that is different from generic randomly
generated graphs. \textcite{Eisenstat2011graphs+quadtree} believes that the
structure of the graphs is important for optimisation problems, giving the
example of optimising logistics operations for a fleet of vehicles where
algorithmic performance greatly differs from that on generic graphs. Uniform
planar graphs are a reasonably realistic way to represent road networks
\parencite{Eisenstat2011graphs+quadtree, Masucci2009graphs+london}. A way to
generate random planar graphs is suggested by
\textcite{Brinkmann2007graphs+generate}, including software called
\textit{plantri}.

Shortest paths in graphs can be calculated by Dijkstra's algorithm
\parencite{Cormen2009algorithms}. However, calculating all possible shortest
paths is not necessary as some routes could never be travelled. Furthermore,
that may be infeasible in a very large network as the number of paths grows
exponentially. Online calculation of a shortest path between two nodes in a
graph can be efficiently done using \textit{A-star} algorithm by
\textcite{Hart1968paths}.

The network was implemented by using a graphing gem \textit{plexus}
\parencite{Plexus}. An alternative graph library considered was \textit{RGL}
\parencite{Rgl}. Unlike RGL, Plexus had not implemented an algorithm to check
for connectivity of directed graphs which could potentially be an issue.
However, RGL was no longer in active development and Plexus natively supports
numerous graph algorithms, including A-star shortest path search that was
mentioned above.


\subsubsection{Taxi}
\label{sec:implementation:software:taxi}

The taxi agent is arguably the most important component of the simulation. It
stores its location and reachable destinations in the environment, the last
reward received, a list of possible prices, and its costs. Taxi initialises the
Q-Learner (see Section \ref{sec:implementation:ai}) with an initial state and a
list of available actions that depend on the location, list of available prices
and presence of a passenger.

A taxi is called to act by the World, when it processes an action and updates
internal parameters. The Q-Learner is updated with the reward for taking the
action, and the taxi is marked as engaged for as long as the action takes. Then
taxi's location and total profit are updated, and it is decided if the new
location has a passenger present. A new action is then chosen by the Q-Learner,
which will be executed the next time the taxi is called to act.

Action is a subclass of Taxi. Each action has a known type: \textit{wait},
\textit{drive} to a location without transporting a passenger or \textit{offer}
a fare for transporting a passenger to a location. All of the action types have
a known \textit{fixed cost}, \textit{variable cost} (not used for
\textit{wait}) and \textit{units} of time the action takes. The costs and units
are used to calculate a cost for action. \textit{Drive} or \textit{offer}
actions additionally have a known destination, and \textit{offer} action can
has different costs depending on whether the offer was accepted by a passenger.
When the taxi starts an action, it is marked as busy.

Each location has two states that the taxi can discern -- without a passenger
present or with a passenger going to a certain location. Each state is
essentially a tuple of \(\left \langle location, destination \right \rangle\)
where the destination can be empty.


\subsubsection{Passenger}
\label{sec:implementation:passenger}

As was identified in Section \ref{sec:requirements:passenger}, passengers have
a set of characteristics and can passively interact with taxis. As each
characteristic is potentially a complex object with different values and
probability distributions, it was decided to implement it as a subclass of
Passenger class. Passengers agents are generated on demand if a taxi is
present, according to a location's probability of having a passenger.

These software classes were designed with the goal of eventually having user-
customisable passenger details and characteristics. However, due to time
pressure and higher priority goals, the passenger details and characteristics
are fixed and uniform.


\subsubsection{Q-Learner}
\label{sec:implementation:ai}

As described in section \ref{sec:requirements:ai}, Q-Learner is a simple and
adequate reinforcement learning method fit for the project. Of course, the very
basic Q-Learner will need to be extended to accommodate exploration in this
particular case of having an active agent (see Section
\ref{sec:literature:ai:exploration}). Therefore an exploration constant
\(\varepsilon\) needs to be used for action selection, and it can stay fixed as
the horizon for the simulation will normally be unknown.

A further addition required for the Q-Learner is a step-size function. This
function is used to adjust for the bias of previous estimates. As a state is
being visited more and more, any new experiences are less and less important
compared to the old experiences, therefore the temporal difference weights
should be adjusted less. This issue was discussed in more detail by
\textcite{Sutton1994ai+stepsize}, where three different algorithms are
introduced for optimising the step sizes for improved reinforcement learning
performance. Nevertheless, the policy will still converge even if the step size
simply decays (i.e \(\lim_{n \to \inf}\alpha(n) = 0\) where \(n\) is the number
of times a state has been visited). Therefore for simplicity the function can
be fixed as \(\alpha(n)=\frac{1}{n}\).

When the step size function is added to a basic Q-Learner, the algorithm
adapted from \textcite{Sutton1998ai+reinforcement} can be used as shown in
Algorithm \ref{algorithm:q}. This algorithm updates action-state pair value
estimates, and the learner can then select the optimal action depending on the
exploration function (action selection algorithm is not shown here). The
required inputs for initialisation and for further operation of the algorithm
are clearly listed.

\begin{algorithm}
  \caption{
  Q-learning. Algorithm that needs to be called after each transition. 
  Adapted from \textcite{Sutton1998ai+reinforcement}. 
  \label{algorithm:q}}

  \begin{algorithmic}[1]
    \Require
      \Statex $s$ is the last state,
      \Statex $s'$ is the next state,
      \Statex $a$ is the last action,
      \Statex $A(s)$ is a set of actions for a state,
      \Statex $R$ is the immediate reward received,
      \Statex $Q$ is an array hosting the current action-value estimate
      \Statex $H$ is the visit history of state-action pairs,
      \Statex $\alpha$ is a step-size function,
      \Statex $\gamma$ is a discount factor.
      \Statex Policy $Q(s, a)$ is initialised arbitrarily
      \Statex History $H$ is empty
    \Function{Q\_learning}{$s,a,R,a'$}
      \State $\delta \gets R + 
              \gamma \cdot max_{a' \in A(s')} Q(s', a') - Q(s, a)$
      \State $Q(s, a) \gets Q(s, a) + \alpha \cdot \delta$
      \State $H(s, a) \gets H(s, a) + 1$
      \State \Return $Q, H$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\subsubsection{Limitations and Issues}
\label{sec:implementation:software:issues}

Performance limits and issues were discussed in Section
\ref{sec:implementation:testing:performance}.

One of the main problems with the existing implementation is the non-
transparent management of default variables. These variables are used for
setting some parameters when user input is not supplied, and used for setting
many parameters where user input is not supported yet (e.g. passenger details).
At the moment these default settings are stored in their respective class
source code files. This approach is infeasible because it complicates
maintainability and configurability of the system. This issue could be solved
by managing these values in a single file. To compensate for the loss of
context, this file would need to be commented similarly to a user manual.

A less important issue is that the taxi action sequence does not mimick real
world processes. In the simulation the taxi executes actions, travels and
selects the next actions immediately while it is only rendered inactive for
some time afterwards; this causes no problems now but could cause issues if
multiple taxi agents are introduced.

User interface is primitive and requires the user to be familiar with operating
a command line terminal. Additionally, there are no progress indicators shown
to the user when the simulation is running.

Finally, Ruby is a single threaded language, which could be problematic for a
future extension to a multi-agent system.

