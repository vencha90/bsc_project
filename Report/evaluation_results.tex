(15, 3200 w)

Has the student carried out an evaluation?

Does the student present the results of the evaluation clearly and in a logical
manner?

Does the student explain the problems and difficulties found?

Does the student demonstrate an understanding and interpretation of results and
their significance?

Is there any critical evaluation of the project relative to the achievements of
related works?

Does the student present a personal reflection of what has been achieved and
not achieved in the project?

Has the student suggested future work?

\todo{remove notes}

\subsection{Simulation}
\label{sec:results}

\subsubsection{Inputs}

The experiments and their analysis can be repeated by following the
instructions in User Manual in Appendix \ref{sec:user_manual}. The result
datasets can be found in the open data set mentioned in
\ref{sec:intro:overview}.

Initially a baseline of inputs were chosen without any intentional constraints
on the environmental conditions. These inputs are shown in Table
\ref{table:input_data} and the full result data can be found in
\texttt{standard} directory in the data set.

\begin{table}
\begin{tabular}{ | l | l | }
  \hline
  Parameter & Value \\ \hline
  Time Limit & 1 000 000 \\
  Road network & with 6 locations, distances between 1-5 \\
  Passengers' expected fare & 20 \\
  Q-Learner's greediness \(\varepsilon\) & 0.9 \\
  Q-Learner's discount factor \(\gamma\) & 0.9 \\
  Q-Learner's default value estimate & 0 \\
  Taxi's variable price range & 5-100 (integers) \\
  Taxi's benchmark price & 20 \\
  \hline
\end{tabular}
\caption{
  Simulation inputs
  \label{table:input_data}
}
\end{table}

The simulation was additionally run for control purposes, varying one input
parameter at a time. These runs are summarised in Table
\ref{table:inputs:control}, displaying the parameter in question and its
changed value, as well as the directory in the open data source where the
results are located.

\begin{table}
\begin{tabular}{ | l | l | l | }
  \hline
  Parameter & Value & Result Directory \\ \hline
  No changes & - & \texttt{standard} \\
  Benchmark price & 15 & \texttt{lower\_benchmark} \\
  Benchmark price & 30 & \texttt{higher\_benchmark} \\
  Q-Learner's default value estimate & 10 & \texttt{value\_estimate\_10} \\
  Q-Learner's default value estimate & 100 & \texttt{value\_estimate\_100} \\
  Q-Learner's default value estimate & -10 & \texttt{value\_estimate\_neg10} \\
  Q-Learner's default value estimate & -100 & \texttt{value\_estimate\_neg100} \\
  Q-Learner's discount factor & 0.4 & \texttt{lower\_discount} \\
  Q-Learner's greediness & 0.4 & \texttt{lower\_greediness} \\
  Taxi's price range & 5 - 20 & \texttt{lower\_range} \\
  Taxi's price range & 20 - 100 & \texttt{higher\_range} \\
  Time limit & 50 000 & \texttt{limited\_time} \\
  \hline
\end{tabular}
\caption{
  Control parameter changes
  \label{table:inputs:control}
}
\end{table}

Besides the configurable inputs, some simulation parameters used defaults
fixed in the source code. Passenger characteristics were manually adjusted so
that the probability of a passenger accepting a fare would be about 75\% for
their expected fare, as in preceding test simulation runs it was experimentally
determined that the formula in Section \ref{sec:requirements:passenger}
resulted in this probability being lower than 10\%.


\subsubsection{Profitability}
\label{sec:results:profits}
\todo{add new results}

Full results and a detailed simulation analysis can be found included in the
project's data set at the web address specified in Section
\ref{sec:intro:online}. This data is organised by simulation runs as identified
in Table \ref{table:inputs:control}, and each simulation run has two parts -- variable
pricing and benchmark.

In all simulation runs, the variable pricing approach performed significantly
better. The summary of total profits are shown in Figure
\ref{figure:results:total_profit}, ordered descending from left to right by
total profit in the simulation using variable fares.

\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{../figures/total_profit}
  \caption{
    Summary of Total Profits in Simulation Results
    \label{figure:results:total_profit}
  }
\end{center}
\end{figure}

\todo{either add more results or remove the extra spaces. use average out of 3?}

Before any result analysis is started, it is important to remember from
\ref{sec:literature:ai:mdp:methods} that Q-Learner if implemented and
configured correctly bound to converge to an optimal policy over time.
Therefore these results mostly reveal the effects of the environmental
configuration and constraints on taxi agent. Similarly, whether a taxi agent
made a profit or a loss is not at all important for these results. The actual
amount of profit/loss depends on the simulation inputs that were not created to
resemble a realistic situation.

The most profitable simulation runs were the runs with a higher fixed benchmark
price and the run with default settings. The worst performance (when adjusted
for the same timescale) was shown by the simulation with a limited timeframe,
which most likely did not have sufficient time to find a near-optimal policy.

Interestingly, the third most profitable was the simulation where the agent was
limited to a price range lower than the passenger's expected fare and lower
than its corresponding fixed benchmark price. This beyond doubt proves that a
flexible approach is more profitable than sticking to a fixed fare price
structure. However, the simulation with a higher variable price range seems to
have performed badly compared to the lower price range, although it is still
much more profitable than the benchmark simulation.

The effect of initial value estimates on the learner seems to have an important
role when they are significantly overvalued. The simulation run 'Value Estimate
100' when comparing the variable price results fared almost as badly as the
simulations with lowered discount factor and greediness that are impaired in
turn by overly focusing on current rewards and by selecting a random action
very often. Additionally the overvalued estimates resulted in an exceptionally
low profit for the fixed price simulation. However, when the value estimates
were close to the actual estimates or significantly undervalued, the simulation
results show only small performance differences that could easily be explained
by randomness.

Emphasizing that in this simulation taxi's cost for waiting was 1 and time
limit was 1 000 000, the cost of a taxi not operating at all would be 1 000
000. It can be noticed that in all but one case the simulation with fixed
pricing generated larger losses than if the agent had been inactive, suggesting
that the reinforcement learning is not working to the taxi's benefit.

Nevertheless, these results strongly suggest that in identical conditions
having a choice to charge a varied range of fare prices is advantageous to a
taxi's profitability, and that Q-Learning can successfully be used for setting
the fare prices.


\subsubsection{Performance of Reinforcement Learning}
\label{sec:results:ai}
\todo{add new results}
\todo{include results in appendices or not?}

Analysis of variance was performed on each simulation's dataset (meaning that
benchmark and variable pricing runs were analysed separately), and a linear
trend of reinforcement learning agent's observed rewards extracted. The results
are available at the web address specified in Section \ref{sec:intro:online}

All bar one of the simulation runs show a statistically significant (95\%
confidence) increase of rewards for variable pricing taxi agents. These reward
trends are shown in Figure \ref{figure:results:variable_rewards}. The agent in
the simulation run with a lower range of prices might have settled on a sub-
optimal policy for a large part of its lifetime, hence the trend of decreasing
reward.

\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{../figures/reward_trend_variable}
  \caption{
    Trend of Rewards (Variable Pricing)
    \label{figure:results:variable_rewards}
  }
\end{center}
\end{figure}

Data runs with a fixed price paint a similar picture (see Figure
\ref{figure:results:fixed_rewards}) with most agents steadily increasing their
rewards. However, there is an outlier here as well: the simulation with a
significantly overvalued initial value estimate failed to make significant
improvement over the course of the simulation. Unsurprisingly, this was the
least profitable simulation with a fixed price as can be seen in
\ref{figure:results:total_profit} and is described in Section
\ref{sec:results:profits}.

\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{../figures/reward_trend_fixed}
  \caption{
    Trend of Rewards (Fixed Pricing)
    \label{figure:results:fixed_rewards}
  }
\end{center}
\end{figure}

\subsection{Summary}

To conclude, the results suggest that the taxi agent was successful at finding
a policy that increases the average reward when it can choose the price freely.
However, the results clearly reveal the importance of setting the environmental
conditions and constraints, which can have a large influence on the
effectiveness of the reinforcement learning.

Worryingly, there was a simulation where the agent showed no change in
rewards. This lack of change raises questions about the correctness of the
software, as the policy was clearly sub-optimal but the rewards did not change
unlike in other simulation runs. There is a possibility that this was caused by
a bug in the implementation, although the remainder of the runs seem to have
been successful and correct.
