\subsection{Testing}
\label{sec:implementation:testing}

Section \ref{sec:implementation:testing:automated} outlines the testing
strategy and how Taxi-sim benefited from automated testing. Besides automated
tests, user testing was performed when the software was nearing completion
(Section \ref{sec:implementation:testing:user}). Finally, performance testing
and the known performance limitations are discussed in Section
\ref{sec:implementation:testing:user}.

\subsubsection{Automated Tests}
\label{sec:implementation:testing:automated}

Test-Driven Development (TDD) was introduced in Section \ref{sec:design:tdd}.
The TDD process was not followed to the letter. Firstly, in some cases with
unfamiliar and unknown problems tests and code were written intermittently as
uncertainty was slowly reduced. Another reason for not following TDD was the
relaxed requirements for user interface, where development alternated between
writing tests first and code first while trying to find the easiest solutions
to comply with requirements.

Most of the tests used are black-box unit tests. These tests allow internal
refactoring of software without (theoretically) affecting the overall system
functionality. As expected, black box testing was not sufficient for testing
the Q-Learner where white-box testing was utilized. Additionally, the Taxi
class was white-box tested to an extent, potentially suggesting that it could
have been refactored in separate modules. Integration tests were used for each
top-level class, for example, integration tests for Taxi class tested how it
integrates Action and State classes.

At the time of submission this project's automated test coverage according to
Simplecov was 99.72\% (1053/1056 lines of code covered). Of course, despite the
RSpec tests and Simplecov test coverage, they in no way guarantee correct and
effective code and could even lead to a false sense of security. Nevertheless,
it is very likely that usage of Simplecov has improved overall code quality as
it did identify some code branches that had not been covered by automated
tests.


\subsubsection{User Testing}
\label{sec:implementation:testing:user}

User testing was done when software had matured enough to have a fixed
interface implementation and a basic User Manual. A computer-literate user with
minimal experience in using command-line applications was tasked with using the
software according to the User Manual (Appendix \ref{sec:user_manual}). The
user found no problems with the software and was able to run the simulation
from the instructions alone. After suggestions from the user, example input and
output files were added to the user manual, as the user found that the work
flow was complicated by looking at examples in external files.

The user was unimpressed by the complex installation process and the usage of
virtualisation. However, the user admitted that it was preferable if facing an
alternative of possible compatibility issues.

\subsubsection{Performance Testing}
\label{sec:implementation:testing:performance}

Only one performance issue was noticed during the usage of the system: A-star
search was chosen for calculating distances between locations. However, this
required repeated graph traversal in the naive form that it was implemented.
This oversight was quickly mitigated by storing the calculated distances in
memory and only traversing the graph for unknown distances.

The main expected software limitation is caused by the dimensionality of state-
action space for Q-Learner. After a certain system-dependent thresholds it is
likely to exhaust available random access memory and require writing to and
reading from hard drive, thus significantly reducing simulation speed. State-
action space depends on the size of the road network and the range of prices
available to the taxi. To be exact, the dimensionality equals \( (n-1) \cdot n
\cdot p\), where \(n\) is number of locations, \(n-1\) is the number of
reachable locations and \(p\) is the number of available prices. Therefore the
main factor limiting the performance of Taxi-sim is hardware and RAM in
particular (assuming the software implementation has no unknown bugs) -- this
has to be kept in mind when specifying inputs for simulations.

A final less serious issue some users might run into is the considerable size
of simulation log files that have a data row for each time unit of the
simulation, running in hundreds and thousands of megabytes for long
simulations. Managing this issue is covered in User Manual in Appendix
\ref{sec:user_manual}.
