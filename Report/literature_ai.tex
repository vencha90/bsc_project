\subsection{Reinforcement Learning}
\label{sec:ai}

\subsubsection{Basics}

Reinforecement learning is software agents being rewarded or punished
(receiving negative reward) for their actions so that they can hopefully learn
an optimal policy for acting in some environment, maximising the economic
outcome (\textbf{utility}).

\textbf{Optimal policy} is defined as the mapping of actions from any possible
state to the highest utility outcome from that state. Thus for each state there
is a maximum utility that can be reached. Agents have a set of possible actions
that they can take, depending on the state they are in. Agents also have a set
of rules for what they can observe from the environment. \textbf{Transition
model} is the description of how applying actions to states result in new
states. All of the states, actions and the transition models together define
the \textbf{state space}. \parencite{Russell2010ai+modern,
Sutton1998ai+reinforcement}

\textbf{Reward} is the relative utility (positive or negative) what an agent
gets for being in a certain state, and the goal of the agent is to maximise the
cumulative reward over time. The cumulative reward is known as the
\textbf{value function} or \textbf{utility function}, giving a long-term
outlook in contrast to immediate rewards. Searching and learning the value
function is central to reinforcement learning. \parencite{Russell2010ai+modern,
Sutton1998ai+reinforcement}

Evolutionary methods \parencite{Sutton1998ai+reinforcement} also known as
genetic algorithms \parencite{Russell2010ai+modern} are a branch of artificial
intelligence that could potentially be used for solving reinforcement learning
problems. Evolutionary methods do not learn from individual experiences, and
therefore \textcite{Sutton1998ai+reinforcement} do not recommend using them for
reinforcement learning problems, although they agree that evolutionary methods
could be useful in specific cases. Similarly \textcite{Russell2010ai+modern}
states that genetic algorithms do not cope well with increasing complexity and
are the most useful in optimisation problems.


\subsubsection{Markov Decision Processes}

An agent having the \textbf{Markov property} means that only the current state
of the agent matters to the probability distribution of future states i.e. any
other states and actions do not influence the future state. Even in cases when
the Markov property does not strictly apply, often approximations can be made
and reinforcement learning theory still applies.
\parencite{Sutton1998ai+reinforcement}

A process satisfying the Markov property is a
\textbf{Markov decision process (MDP)}. It consists of the state space and
reward functions. It can be seen that all MDPs have a policy, the one with the
highest expected utility called the \textbf{optimal policy}. It is important to
note that it is the \textit{expected} utility because the environment could be
(and usually is) stochastic. \parencite{Russell2010ai+modern}

Considering MDPs over time, two cases can be distinguished: finite and infinite
horizon. While the optimal action in a given state could change when having a
finite horizon, it never would with an infinite horizon as there is no
differing time pressure. To solve MDPs with an infinite horizon and no terminal
states, rewards need to be discounted. Finding an optimal policy is not always
possible due to the large state space, therefore approximation may be needed.
Approximation can give good results because reaching many states has a very
small probability and they have very low effect on the overall utility of a
policy. \parencite{Russell2010ai+modern}

The simple model of MDPs assumed that the environment was perfectly observable
i.e. the agent knew which state it is in at all times. This assumption is
unrealistic in the real world -- for example, a taxi driver does not know
whether a passenger will accept their offered fare. To account for this,
\textbf{Partially observable Markov decision processes} (POMDP) add the notion
of a \textbf{sensor model} specifying the probabilities of perceiving evidence.
Therefore now the set of states is a set of probability distributions for a
POMDP -- the belief state. \parencite{Russell2010ai+modern}

Therefore it can be concluded that the taxi in the problem in this paper is in a POMDP situation. There is a clear reward function between states - the fare. However, the taxi does not know the exact state it is in i.e. how much the passenger is willing to pay.

\subsubsection{Solving POMDPs}


HEREHEREHERE


\subsubsection{Solving Reinforcement Learning Problems}

Three groups of approaches for solving reinforcement learning problems (finding
optimal policies) are covered by both \textcite{Russell210ai+modern} and
\textcite{Sutton1998ai+reinforcement} can be categorised as follows: model-
free methods (examined in more detail in Section \ref{sec:ai:model_free} and
model-based methods (Section \ref{sec:ai:model_based}). To understand why this
distinction is made, their common background needs to be investigated first.

The simplest naive approach to solve reinforcement learning problems is
\textbf{direct utility estimation}. It uses the fact that utility of a state is
the expected total reward from that state onward, called the reward-to-go.
(Widrow and Hoff, 1960 cited by \textcite{Russell2010ai+modern}) After a large
number of estimations giving samples for the states, the observed reward-to-go
is likely to converge to the actual utility of a state. However, this approach
is inefficient, mainly because it ignores that utilities of states are related
to each other. \parencite{Russell2010ai+modern}

Dynamic programming methods, unlike direct utility estimation, takes in
account the interdependence of utilities of states by learning the transition
model that connects them, and uses dynamic programming to solve the
corresponding Markov decision process. However, this requires a perfectly known
model of the environment which in practice is infeasible. Dynamic programming
is also computationally expensive. The two most popular dynamic programming
methods are value iteration and policy iteration. Dynamic programming is the
basis for both the developed model-based and model-free methods looked at here.
\parencite{Sutton1998ai+reinforcement}

The \textbf{generalized policy iteration} (GPI) (policy iteration ins
\textcite{Russell2010ai+modern}) has been proven to converge to optimal
policies and value functions when using dynamic programming methods. GPI
repeatedly alternates between these two steps: policy evaluation and policy
improvement. Policy evaluation calculates a value function based on the current
policy and updates the existing value function to be closer to the newly
calculated value function. Policy improvement uses the updated value function
to calculate a new policy and then updates the existing policy to be closer to
the newly calculated policy. Most reinforcement learning problems use this
algorithm. \perencite{Sutton1998ai+reinforcement}


\subsubsection{Model-free methods}
\label{sec:ai:model_free}

There are two categories of model-free agents: reflex agents and Q-learning
agents. Q-learning agents learn an action-utility function, giving the expected
utility of taking an action in a given state. Reflex agents learn a policy that
is a direct mapping from states to actions.

Reflex agents are more primitive and 


\subsubsection{Model-based methods}
\label{sec:ai:model_based}


\subsubsection{Temporal difference methods}
\label{sec:ai:temporal}

\textbf{Temporal difference (TD)} does not use a transition model and therefore
is simpler and requires less computation than dynamic programming, alas at a
price of slower learning. TD works by adjusting the utility estimates based on
the differences observed in the last state transition, and over time the
\textit{average} utility values converge to the correct values. To ensure that
utility values in TD converge to the correct value, visited states can be
stored and their repeated impact reduced. \parencite{Russell2010ai+modern}


\subsubsection{Synergy}
\label{sec:ai:synergy}
