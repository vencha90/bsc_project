\subsection{Reinforcement Learning}
\label{sec:ai}

\todo{more critical evaluation?}
\todo{too much POMDP?}

\subsubsection{Basics}
\label{sec:ai:basics}

Reinforecement learning is software agents being rewarded or punished
(receiving negative reward) for their actions so that they can hopefully learn
an optimal policy for acting in some environment, maximising the economic
outcome (\textbf{utility}).

\textbf{Optimal policy} is defined as the mapping of actions from any possible
state to the highest utility outcome from that state. Thus for each state there
is a maximum utility that can be reached. Agents have a set of possible actions
that they can take, depending on the state they are in. Agents also have a set
of rules for what they can observe from the environment. \textbf{Transition
model} is the description of how applying actions to states result in new
states. All of the states, actions and the transition models together define
the \textbf{state space}. \parencite{Russell2010ai+modern,
Sutton1998ai+reinforcement}

\textbf{Reward} is the relative utility (positive or negative) what an agent
gets for being in a certain state, and the goal of the agent is to maximise the
cumulative reward over time. The cumulative reward is known as the
\textbf{value function} or \textbf{utility function}, giving a long-term
outlook in contrast to immediate rewards. Searching and learning the value
function is central to reinforcement learning. \parencite{Russell2010ai+modern,
Sutton1998ai+reinforcement}

Evolutionary methods \parencite{Sutton1998ai+reinforcement} (also referred to
as genetic algorithms by \textcite{Russell2010ai+modern}) are a branch of
artificial intelligence that could potentially be used for solving
reinforcement learning problems. Evolutionary methods do not learn from
individual experiences, and therefore \textcite{Sutton1998ai+reinforcement} do
not recommend using them for reinforcement learning problems, although they
agree that evolutionary methods could be useful in specific cases. Similarly
\textcite{Russell2010ai+modern} states that genetic algorithms do not cope well
with increasing complexity and are the most useful in optimisation problems.


\subsubsection{Markov Decision Processes (MDPs)}
\label{sec:ai:mdp}

An agent having the \textbf{Markov property} means that only the current state
of the agent matters to the probability distribution of future states i.e. any
other states and actions do not influence the future state. Even in cases when
the Markov property does not strictly apply, often approximations can be made
and reinforcement learning theory still applies.
\parencite{Sutton1998ai+reinforcement}

A process satisfying the Markov property is a \textbf{Markov decision process
(MDP)}. It consists of the state space and reward functions. It can be seen
that all MDPs have a policy, the one with the highest expected utility called
the \textbf{optimal policy}. It is important to note that it is the
\textit{expected} utility because the environment could be (and usually is)
stochastic. \parencite{Russell2010ai+modern}

Considering MDPs over time, two cases can be distinguished: finite and infinite
horizon. While the optimal action in a given state could change when having a
finite horizon, it never would with an infinite horizon as there is no
differing time pressure. To solve MDPs with an infinite horizon and no terminal
states, rewards need to be discounted using a discount parameter \(\gamma\).
Finding an optimal policy is not always possible due to the large state space,
therefore approximation may be needed. Approximation can give good results
because reaching many states has a very small probability and they have very
low effect on the overall utility of a policy. \parencite{Russell2010ai+modern}

The simple model of MDPs assumed that the environment was perfectly observable
i.e. the agent knew which state it is in at all times. This assumption is
unrealistic in the real world -- for example, a taxi driver does not know when
they will find a passenger and if the passenger will accept their offered fare.
To account for this, \textbf{Partially observable Markov decision processes}
(POMDP) add the notion of a \textbf{sensor model} specifying the probabilities
of perceiving evidence. Therefore now the set of states is a set of probability
distributions for a POMDP -- the belief state as discussed by
\textcite{Russell2010ai+modern}. \textcite{Spaan2012ai+pomdp} go in more detail
and specify that to form belief models, an agent needs some form of memory to
record observations. With continuous states or with large planning horizons
storing the history directly is infeasible. For the transformation of POMDP to
a MDP \textcite{Spaan2012ai+pomdp} suggest a method summarising agents'
observation history to a belief vector of continuous probability
distributions\(b\), described by Stratonovich, 1960, Dynkin, 1965 and Åström,
1965. This method will be discussed in more detail in Section
\ref{sec:ai:pomdp}.

According to \textcite{Spaan2012ai+pomdp}, fully observavle finite MDPs can be
formally defined as follows. Time has discrete steps, and an agent has to
execute an action at each step (unless the action lasts multiple steps). The
environment is desccribed by a set of states \(S = \{s_1,..,s_n\}\), and the
agent has a set of actions \(A = \{a_1,..,a_n\}\). Each time the agent takes an
action \(a\) in a state \(s\), the environment transitions to a new state
\(s'\) according to a probabilistic transition function \(T(s,a,s')\), and the
agent receives an immediate reward \(R(s,a,s')\).

It can be concluded that the problem of this paper could be a POMDP and could
be the simpler MDP, depending on the specification. There is a clear reward
function between states - the fare a passenger pays. The agent (taxi) may or
may not be aware of the full model of the environment. If passengers are
waiting, taxis not in the immediate vicinity will now be aware of it.
Considerations of solving MDPs will be examined in Section
\ref{sec:ai:mdp:solving}, and POMDPs will be discussed in more detal in
sections \ref{sec:ai:pomdp}, \ref{sec:ai:pomdp:model:based} and
\ref{sec:ai:pomdp:model:free}.


\subsubsection{Solving MDPs}
\label{sec:ai:mdp:solving}

Two groups of approaches for solving reinforcement learning problems (finding
optimal policies) are covered by both \textcite{Russell2010ai+modern} and
\textcite{Sutton1998ai+reinforcement} and can be categorised as model-free
methods and model-based methods (discussed in Section
\ref{sec:ai:mdp:methods}). However, both types of methods have a
common background that first needs to be looked at in this section.

The simplest naive approach to solve reinforcement learning problems is
\textbf{direct utility estimation}. It uses the fact that utility of a state is
the expected total reward from that state onward, called the reward-to-go.
(Widrow and Hoff, 1960 cited by \textcite{Russell2010ai+modern}) After a large
number of estimations giving samples for the states, the observed reward-to-go
is likely to converge to the actual utility of a state. However, this approach
is inefficient, mainly because it ignores that utilities of states are related
to each other. \parencite{Russell2010ai+modern}

Dynamic programming methods, unlike direct utility estimation, take in account
the interdependence of utilities of states by learning the transition model
that connects them, and uses dynamic programming to solve the corresponding
Markov decision process. However, this requires a perfectly known model of the
environment which in practice is infeasible. Dynamic programming is also
computationally expensive. The simplest dynamic programming method is value
iteration. Dynamic programming is also the basis for the advanced methods
looked at in Section \ref{sec:ai:mdp:methods}.
\parencite{Sutton1998ai+reinforcement}

Dynamic programming methods are based on generalized policy iteration (GPI)
which has been proven to converge to optimal policies and value functions. GPI
repeatedly alternates between these two steps: policy evaluation and policy
improvement. Policy evaluation calculates a value function based on the current
policy and updates the existing value function to be closer to the newly
calculated value function. Policy improvement uses the updated value function
to calculate a new policy and then updates the existing policy to be closer to
the newly calculated policy. Most reinforcement learning problems use GPI.
\parencite{Sutton1998ai+reinforcement}


\subsubsection{Traditional MDP Methods}
\label{sec:ai:mdp:methods}

Two groups of methods are distinguished by
\textcite{Sutton1998ai+reinforcement}: \textit{off-policy} and \textit{on-
policy}. Off-policy methods are called that way because they do not operate
based on the estimated optimal policy, but have a behaviour policy that could
even be completely unrelated to the estimated optimal policy. This allows for
exploration, while an on-policy agent operates on and estimates a single
policy. If an agend does not explore, it can easily settle on a sub-optimal
policy.

\textbf{Temporal difference (TD)} methods do not use a transition model and
therefore are simpler and require less computation than basic dynamic
programming, alas at a price of slower learning. TD works by adjusting the
utility estimates based on the differences observed in the last state
transition, and over time the \textit{average} utility values converge to the
correct values. To ensure that utility values in TD converge to the correct
value, visited states can be stored and their repeated impact reduced.
\parencite{Russell2010ai+modern}

To take in account more of the observed history than just the last state
transition, TD(\(\lambda\)) is named by \textcite(Sutton1998ai+reinforcement)
as an improved method. The parameter \(\lambda\) regulates how much of the
transition history is used when updating policy to correct for errors observed
in the last transition. \textcite(Szepesvari2010ai+algorithms) notes that
finding the best value of \(\lambda\) is usually done by trial and error.
However, this only affects the speed at which the method converges to an
optimal policy, and \(\lambda\) can be adjusted as needed when using the
method.

A popular off-policy model-free method is \textbf{Q-learning}, which is based
on the TD approach. A Q-learning agent works by learning an action-value
function for each state that ultimately gives the expected utility of taking a
given action and following the optimal policy thereafter. The optimal policy
can be constructed by simply selecting the action with the highest value in
each state when the action-value function is learned. Q-learning is proven to
converge to optimal policies over time. Unfortunately Q-learning is believed to
be limited in complex environments as it cannot simulate possible courses of
action. However, Q-learning is probably the easiest reinforcement learning
method to implement. \parencite{Russell2010ai+modern}

The space that an agent operates on could be very large, for example, having a
near infinite set of states or actions. In the case of having multiple states
function approximation is suggested by \textcite{Sutton1998ai+reinforcement}
and \textcite{Russell2010ai+modern}. \textcite{Szepesvari2010ai+algorithms}
notes that it is not known if function approximation when used for Q-learning
will converge to optimal policies, although it has been proven to work and
converge to an optimal policy in very specific restricted conditions. A
different approach for continuous state and action spaces using TD actor-critic
Monte Carlo methods is described by \textcite{Lazaric2008ai+smc}, where the
software agent has a separated policy and value function.


\subsubsection{Exploration}
\label{sec:ai:exploration}

A final issue not yet covered is exploration. GPI-based methods (see Section
\ref{sec:ai:mdp:solving}) covered here in their naive forms are greedy,
meaning that they will often converge to a policy optimal for the subset of
states they have experienced (a suboptimal policy)
\parencite{Russell2010ai+modern}. Instead, the agent should find a globally
optimal policy. The agent faces a trade-off between exploration and
exploitation. 

\textcite{Sutton1998ai+reinforcement} describe \(\varepsilon\)-greedy methods
to deal with this trade-off. This simply fixes how inclined an agent is to
choose an action which it considers optimal with a variable \(\varepsilon\),
choosing a random action with a probability \(1-\varepsilon\). If the
environment does not change over time, then it is suggested to reduce
\(\varepsilon\) as the agent is more likely to have found a good policy.

However, taking an action randomly after some experience with the environment
is naive. A common method for discrete action spaces is Boltzmann exploration.
The Boltzmann exploration strategy privileges the execution of actions with
higher estimated utility values, increasing the bias towards greater
exploitation over time. Gaussian exploration is a similar approach for
continuous action spaces. \parencite{Hasselt2012ai+continuous}


\subsubsection{Partially Observable Markov Decision Processes (POMDPs)}
\label{sec:ai:pomdp}

The partial observability of POMDPs can lead an agent perceiving two different
states as similar to take the same action in both, while in fact the agent
should have taken different actions. POMDP captures the partial observability
of the environment in a probabilistic observation model - the belief state. As
the belief state is a probability distribution, it is a continuous state space
where approximations need to be made in order to use reinforcement learning
methods. \parencite{Russell2010ai+modern}

Comparing POMDPs with the definition of MDPs in Section \ref{sec:ai:pomdp}, the
new state \(s'\) cannot be perceived directly. Instead the agent perceives an
observation \(o \in \Omega : \Omega = \{o_1,..,o_m\}\), where \(\Omega\) is the
set of all possible observations. The probability of observing \(o\) in state
\(s\) after executing \(a\) is calculated by an observation function \(O\),
where \(O : S \times A \times \Omega \rightarrow [0,1] \). It is required that
\(\forall s' \in S, a \in A, o \in O ~ O (s', a, o) \geq 0 \) and that
\(\sum_{o \in \Omega}^{} O (s', a, o) = 1 \). It is worth noting that the
action \(a\) is not always required in the definition, as the observation could
be independent of the action i.e. \(O : S \times \Omega \rightarrow [0,1] \).
\parencite{Spaan2012ai+pomdp}

Belief states for POMDPs can be calculated using the method mentioned in
Section \ref{sec:ai:mdp}. Assuming an initial belief state \(b_0\), every time
an agent takes an action \(a\) and observes \(o\), its belief is updated by
this rule: \[ b_{ao}(s') = \frac{ p(o|s',a) }{ p(o|b,a) } \sum_{s \in S}
p(s'|s,a) b(s) \] where probabilities \(p(o|s',a)\) and \(p(o|b,a)\) are
defined by the model and where \[p(o|b,a) = \sum_{s' \in S} p(o|s',a) \sum_{s
\in S} p(s'|s,a) b(s) \] is a normalising constant.
\parencite{Spaan2012ai+pomdp}

Continuous models are a natural way to model many real world problems. Methods
have been developed to solve POMDPs with continuous state spaces, continuous
observation spaces and continuous action spaces. In the continuous cases models
need to have simple parametrisations, and result in finite belief updates and
history of states. \parencite{Spaan2012ai+pomdp}

Reinforcement learning for finite space POMDPs is more developed than for
continuous spaces. Most commonly dynamic Bayesian networks (probabilistic
graphical models) are used. Further methods have been developed to tackle more
complex or specific cases: relational models, hierarchical models structuring
problem solving in multiple levels, and decentralised models for multi-agent
POMDPs. \parencite{Spaan2012ai+pomdp}

Two methods of solving POMDPs are developed based on their approach to dealing
with environment. Model-based methods have an agent using internal
representations of the environment and relating that to their learning. Model-
free method agents do not store any direct representation of the environment,
instead keeping track of their experiences, often in an approximated form.


\subsubsection{Model-based POMDP methods}
\label{sec:ai:pomdp:model:based}

In model-based agent the main consideration is the memory space and
computational complexity of representing and working with multi-dimensional
belief space. There are optimal solutions that are easily computable for small,
discrete POMDPs but infeasible on a larger scale. Therefore some form of
approximation is needed in order to produce scalable methods.
\parencite{Spaan2012ai+pomdp}

Point-based methods work by interacting only with a sample of the belief space
that is reachable, for example, by letting an agent interact with the
environment beforehand in a simulation. Other sampling schemes are proposed as
well and different sampling strategies are recommended for different
situations. Point-based methods allow to get good approximated results for
large problems from a small, representative set of beliefs.
\parencite{Spaan2012ai+pomdp}

\textcite{Spaan2012ai+pomdp} mentions three less successful methods that use
approximation. Grid-based approximations perform value backups for each point
on a grid of belief simplex, and define the value of non-grid points by an
interpolation rule. Policy search methods have demonstrated success in several
cases but often stick to locally optimal policies instead of finding a global
optimum. Finally, heuristic search methods are also used for specific cases.


\subsubsection{Model-free POMDP methods}
\label{sec:ai:pomdp:model:free}

All of the methods previously mentioned in Section
\ref{sec:ai:pomdp:model:based} require a complete knowledge of the POMDP model:
the transition and reward model is needed for all methods, while some of them
require the observation model. Unfortunately none of these are usually known in
the real world. According to \textcite{Spaan2012ai+pomdp}, there are two
approaches to solving model-free POMDPs: direct (memoryless) techniques do not
try to keep track of the POMDP and directly map observations to actions, while
indirect methods try to construct the POMDP from observations.

Q-learning and SARSA that were mentioned in \ref{sec:literature:ai:mdp:methods}
have been adapted to work with POMDPs according to both
\textcite{Russell2010ai+modern, Spaan2012ai+pomdp}. Memoryless policies do not
keep track of the POMDP i.e. they have no internal state. They can be
deterministic mappings \(\pi : \Omega \rightarrow A \) or probabilistic
mappings \(\pi : \Omega \rightarrow \Delta A \). Probabilistic policies cannot
be enumerated and thus have a large policy search space, but have higher
payoffs than deterministic mappings. Methods are named for stationary
deterministic policies, non-stationary deterministic policies and stochastic
policies. An different approcah is hierarchical Q-learning which learns a
subgoal sequence, where each of the subgoals can be reached by a memoryless
policy. \parencite{Spaan2012ai+pomdp}

The memoryless policies are limited because they do not store any history of
actions and observations. Storing the complete history is infeasible with long
horizons. Storing a fixed window of history is also impractical as some
observations may be important and others can be safely discarded. A way to deal
with this issue is extending the history window for potentially worthwile
observations, and ways to do this have been developed. An alternative to POMDPs
suggested for modelling stochastic partially observable environments is
predictively defined representations of state (PSR).
\parencite{Spaan2012ai+pomdp}

PSRs are proven to be at least as compact as POMDPs with a similar
computational complexity, but allow more expressiveness. PSRs define state
differently than POMDPs - state in a PSR represents the complete history of
observations and is sufficient to predict the future distribution of
observations and rewards. Partly due to this representation PSRs allow
increased learnability, representational ability and generalisation. PSRs have
also been proven to be translatable from POMDPs. However, when comparing to
POMDPs, only a few solution methods have been adapted and no completely unique
methods developed, showing no significant improvement over POMDPs. This area is
still relatively young and promising in the future.
\parencite{Wingate2012ai+psr}


\todo{a summary needed?}
