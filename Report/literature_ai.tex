\subsection{Reinforcement Learning}
\label{sec:literature:ai}

\todo{more critical evaluation?}

\subsubsection{Basics}
\label{sec:literature:ai:basics}

Reinforcement learning is software agents being rewarded or punished
(receiving negative reward) for their actions so that they can hopefully learn
an optimal policy for acting in some environment, maximising the economic
outcome (\textbf{utility}).

\textbf{Optimal policy} is defined as the mapping of actions from any possible
state to the highest utility outcome from that state. Thus for each state there
is a maximum utility that can be reached. Agents have a set of possible actions
that they can take, depending on the state they are in. Agents also have a set
of rules for what they can observe from the environment. \textbf{Transition
model} is the description of how applying actions to states result in new
states. All of the states, actions and the transition models together define
the \textbf{state space}. \parencite{Russell2010ai+modern,
Sutton1998ai+reinforcement}

\textbf{Reward} is the relative utility (positive or negative) what an agent
gets for being in a certain state, and the goal of the agent is to maximise the
cumulative reward over time. The cumulative reward is known as the
\textbf{value function} or \textbf{utility function}, giving a long-term
outlook in contrast to immediate rewards. Searching and learning the value
function is central to reinforcement learning. \parencite{Russell2010ai+modern,
Sutton1998ai+reinforcement}

Evolutionary methods \parencite{Sutton1998ai+reinforcement} (also referred to
as genetic algorithms by \textcite{Russell2010ai+modern}) are a branch of
artificial intelligence that could potentially be used for solving
reinforcement learning problems. Evolutionary methods do not learn from
individual experiences, and therefore \textcite{Sutton1998ai+reinforcement} do
not recommend using them for reinforcement learning problems, although they
agree that evolutionary methods could be useful in specific cases. Similarly
\textcite{Russell2010ai+modern} stated that genetic algorithms do not cope well
with increasing complexity and are the most useful in optimisation problems.


\subsubsection{Markov Decision Processes (MDPs)}
\label{sec:literature:ai:mdp}

An agent having the \textbf{Markov property} means that only the current state
of the agent matters to the probability distribution of future states i.e. any
other states and actions do not influence the future state. Even in cases when
the Markov property does not strictly apply, often approximations can be made
and reinforcement learning theory still applies.
\parencite{Sutton1998ai+reinforcement}

A process satisfying the Markov property is a \textbf{Markov decision process
(MDP)}. It consists of the state space and reward functions. It can be seen
that all MDPs have a policy, the one with the highest expected utility called
the \textbf{optimal policy}. It is important to note that it is the
\textit{expected} utility because the environment could be (and usually is)
stochastic. \parencite{Russell2010ai+modern}

Considering MDPs over time, two cases can be distinguished: finite and infinite
horizon. While the optimal action in a given state could change when having a
finite horizon, it never would with an infinite horizon as there is no time
pressure. To solve MDPs with an infinite horizon and no terminal states,
rewards need to be discounted using a discount parameter \(\gamma\). Finding an
optimal policy is not always possible due to the large state space, therefore
approximation may be needed. Approximation can give good results because
reaching many states has a very small probability and they have very low effect
on the overall utility of a policy. \parencite{Russell2010ai+modern}

The simple model of MDPs assumed that the environment was perfectly observable
i.e. the agent knew which state it is in at all times. This assumption is
unrealistic in the real world -- for example, a taxi driver does not know when
they will find a passenger and if the passenger will accept their offered fare.
In this case the environment is partially observable, hence the problem could
be defined as Partially Observable MDP. This case is considered in Section
\ref{sec:literature:ai:pomdp}

According to \textcite{Spaan2012ai+pomdp}, fully observable finite MDPs can be
formally defined as follows. Time has discrete steps, and an agent has to
execute an action at each step (unless the action lasts multiple steps). The
environment is described by a set of states \(S = \{s_1,..,s_n\}\), and the
agent has a set of actions \(A = \{a_1,..,a_n\}\). Each time the agent takes an
action \(a\) in a state \(s\), the environment transitions to a new state
\(s'\) according to a probabilistic transition function \(T(s,a,s')\), and the
agent receives an immediate reward \(R(s,a,s')\).

It can be concluded that the problem of this paper could be a POMDP and could
be the simpler MDP, depending on the specification. There is a clear reward
function between states - the fare a passenger pays. The agent (taxi) may or
may not be aware of the full model of the environment. If passengers are
waiting, taxis not in the immediate vicinity will now be aware of it.
Considerations of solving MDPs will be examined in Sections
\ref{sec:literature:ai:mdp:solving} and \ref{sec:literature:ai:exploration}
with an in-depth overview of methods in Section
\ref{sec:literature:ai:mdp:methods}. Finally, POMDPs that could be important
for the future outlook of this project will be looked at in Section
\ref{sec:literature:ai:pomdp}


\subsubsection{Solving MDPs}
\label{sec:literature:ai:mdp:solving}

Two groups of approaches for solving reinforcement learning problems (finding
optimal policies) were covered by both \textcite{Russell2010ai+modern} and
\textcite{Sutton1998ai+reinforcement} and can be categorised as model-free
methods and model-based methods (discussed in Section
\ref{sec:literature:ai:mdp:methods}). However, both types of methods have a
common background that first needs to be looked at in this section.

The simplest naive approach to solve reinforcement learning problems is
\textbf{direct utility estimation}. It uses the fact that utility of a state is
the expected total reward from that state onward, called the reward-to-go.
(Widrow and Hoff, 1960 cited by \textcite{Russell2010ai+modern}) After a large
number of estimations giving samples for the states, the observed reward-to-go
is likely to converge to the actual utility of a state. However, this approach
is inefficient, mainly because it ignores that utilities of states are related
to each other. \parencite{Russell2010ai+modern}

Dynamic programming methods, unlike direct utility estimation, take in account
the interdependence of utilities of states by learning the transition model
that connects them, and uses dynamic programming to solve the corresponding
Markov decision process. However, this requires a perfectly known model of the
environment which in practice is infeasible. Dynamic programming is also
computationally expensive. The simplest dynamic programming method is value
iteration. Dynamic programming is also the basis for the advanced methods
looked at in Section \ref{sec:literature:ai:mdp:methods}.
\parencite{Sutton1998ai+reinforcement}

Dynamic programming methods are based on generalized policy iteration (GPI)
which has been proven to converge to optimal policies and value functions. GPI
repeatedly alternates between these two steps: policy evaluation and policy
improvement. Policy evaluation calculates a value function based on the current
policy and updates the existing value function to be closer to the newly
calculated value function. Policy improvement uses the updated value function
to calculate a new policy and then updates the existing policy to be closer to
the newly calculated policy. Most reinforcement learning problems use GPI.
\parencite{Sutton1998ai+reinforcement}


\subsubsection{Traditional MDP Methods}
\label{sec:literature:ai:mdp:methods}

Two groups of methods are distinguished by
\textcite{Sutton1998ai+reinforcement}: \textit{off-policy} and \textit{on-
policy}. Off-policy methods are called that way because they do not operate
based on the estimated optimal policy, but have a behaviour policy that could
even be completely unrelated to the estimated optimal policy. This allows for
exploration, while an on-policy agent operates on and estimates a single
policy. If an agent does not explore, it can easily settle on a sub-optimal
policy.

\textbf{Temporal difference (TD)} methods do not use a transition model and
therefore are simpler and require less computation than basic dynamic
programming, alas at a price of slower learning. TD works by adjusting the
utility estimates based on the differences observed in the last state
transition, and over time the \textit{average} utility values converge to the
correct values. To ensure that utility values in TD converge to the correct
value, visited states can be stored and their repeated impact reduced.
\parencite{Russell2010ai+modern}

To take in account more of the observed history than just the last state
transition, TD(\(\lambda\)) is named by \textcite(Sutton1998ai+reinforcement)
as an improved method. The parameter \(\lambda\) regulates how much of the
transition history is used when updating policy to correct for errors observed
in the last transition. \textcite(Szepesvari2010ai+algorithms) notes that
finding the best value of \(\lambda\) is usually done by trial and error.
However, this only affects the speed at which the method converges to an
optimal policy, and \(\lambda\) can be adjusted as needed when some experience
of using the method is known.

A popular off-policy model-free method is \textbf{Q-learning}, which is based
on the TD approach. A Q-learning agent works by learning an action-value
function for each state that ultimately gives the expected utility of taking a
given action and following the optimal policy thereafter. The optimal policy
can be constructed by simply selecting the action with the highest value in
each state when the action-value function is learned. Q-learning is proven to
converge to optimal policies over time. Unfortunately Q-learning is believed to
be limited in complex environments as it cannot simulate possible courses of
action. However, Q-learning is probably the easiest reinforcement learning
method to implement. \parencite{Russell2010ai+modern}

The space that an agent operates on could be very large, for example, having a
near infinite set of states or actions. In the case of having multiple states
function approximation is suggested by \textcite{Sutton1998ai+reinforcement}
and \textcite{Russell2010ai+modern}. \textcite{Szepesvari2010ai+algorithms}
notes that it is not known if function approximation when used for Q-learning
will converge to optimal policies, although it has been proven to work and
converge to an optimal policy in very specific restricted conditions. A
different approach for continuous state and action spaces using TD actor-critic
Monte Carlo methods is described by \textcite{Lazaric2008ai+smc}, where the
software agent has a separated policy and value function.


\subsubsection{Exploration}
\label{sec:literature:ai:exploration}

A final issue not yet covered is exploration. GPI-based methods (see Section
\ref{sec:literature:ai:mdp:solving}) covered here in their naive forms are
greedy, meaning that they will often converge to a policy optimal for the
subset of states they have experienced (a suboptimal policy)
\parencite{Russell2010ai+modern}. Instead, the agent should find a globally
optimal policy. Thus the agent faces a trade-off between exploration and
exploitation i.e. between current profits and future profits.

\textcite{Sutton1998ai+reinforcement} describe \(\varepsilon\)-greedy methods
to deal with this trade-off. This simply fixes how inclined an agent is to
choose an action which it considers optimal with a variable \(\varepsilon\),
choosing a random action with a probability \(1 - \varepsilon\). If the
environment does not change over time, then it is suggested to reduce
\(\varepsilon\) as the agent is more likely to have found a good policy.

However, taking an action randomly after some experience with the environment
is naive. A common method for discrete action spaces is Boltzmann exploration.
The Boltzmann exploration strategy privileges the execution of actions with
higher estimated utility values, increasing the bias towards greater
exploitation over time. Gaussian exploration is a similar approach for
continuous action spaces. \parencite{Hasselt2012ai+continuous}


\subsubsection{Partially Observable Markov Decision Processes (POMDPs)}
\label{sec:literature:ai:pomdp}

\textbf{Partially observable Markov decision processes} (POMDP) add the notion
of a \textbf{sensor model} to specify the probabilities of perceiving evidence.
Therefore now the set of states is a set of probability distributions for a
POMDP -- the belief state as discussed by \textcite{Russell2010ai+modern}.
\textcite{Spaan2012ai+pomdp} go in more detail and specify that to form belief
models, an agent needs some form of memory to record observations. With
continuous states or with large planning horizons storing the history directly
is infeasible.

\textcite{Russell2010ai+modern} noted that to use reinforcement learning
methods on POMDPs, approximations need to be used for the continuous state
space. For the transformation of POMDP to a MDP \textcite{Spaan2012ai+pomdp}
suggested a method developed by Stratonovich (1960), Dynkin (1965) and Åström
(1965): summarising agents' observation history to a belief vector \(b\) of
continuous probability distributions.

Q-learning and SARSA that were mentioned in \ref{sec:literature:ai:mdp:methods}
have been adapted to work with POMDPs according to both
\textcite{Russell2010ai+modern, Spaan2012ai+pomdp}, alas with limited success.
These POMDP methods are extensions of the basic MDP methods much like POMDPs
are extended MDPs, therefore an evolutionary transformation of this project
from using MDP to using POMDP is likely to be possible if ever needed. A
detailed overview of these and other POMDP methods is given by
\textcite{Spaan2012ai+pomdp}.
